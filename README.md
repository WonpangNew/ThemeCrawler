# ThemeCrawler
This is a small topic crawler, relying on regular expressions to get the URL in each page, and the use of cosine distance filter URL.<br>
该爬虫包括以下三个模块：
1.初始种子和关键词模块：用于确定此次要进行爬取的主题和主题相关的关键词
2.主题相关度分析模块：主题爬虫的核心模块,它决定页面的取舍
3.排序模块：决定网页抓取的顺序，通过对网页进行一个优先级排序，每次都从优先级高的网页开始抓取，可以保持主题不偏移。
初始化完种子和关键词后，爬虫就会从初始种子开始爬取网页，然后获取其链接，将链接添加到待抓取优先队列中。之后，爬虫就进入正轨，开始从优先队列中获取链接，然后抓取网页判断网页的相关度以决定是否丢弃该网页。如果网页相关度大于阈值，则再将其添加到优先队列中等待爬取。
  
<h1>1.初始种子和关键词模块</h1>
我们选择的主题是篮球，那么初始种子选择的就是各大篮球门户网站。<br>
采用的方法是用关键词集来确立主题,其中每个关键词拥有指定的不同权值。权值的设置有两种方法:手工设置和特征提取。综合二者的优点得出关键词的权值方法:<br>
(1)	手工(主要是通过咨询领域专家获取)设置一组关键词并分配权值;<br>
(2)	用这组关键词到搜索引擎中查找出部分对应的网页;<br>
(3)	按权值的比例选取一定数量的网页;<br>
(4)	用这些网页组成的集合作为特征提取程序的输入,得到一组新的关键词及权值。<br>
<h1>2.主题相关度分析模块</h1>
暂时采用余弦距离.指定一个阈值r ,当cos<α,β>   r时就可以认为该页面和主题是比较相关的, r的取值需要根据经验和实际要求确定,如果想获得较多的页面,可以把r设小一点,要获得较少的页面可以把r设的大一点。在程序中我们暂时默认将阈值r设置为0.9。
<h1>3.排序模块</h1>
排序模块的作用是对网页的重要程度进行排序,把价值高的网页排到前面,以便它们更容易地被选择到。影响网页排序的因素很多,通过主题相关度的计算已经能够得到一个比较合理的排序,但是为了得到一个更加合理的排序,必须辅助考虑其他因素。一个因素是考虑该网页的链接个数，如果一个网页包含有很多链接，说明该网页更多的是类似于hao123这样的门户网站，这种网页的优先级则低于包含大量文本的主题性网页。所以我们在获得余弦值后将其除以该网页包含的链接个数来获取其更加精确的优先级。<br>
<h1>4.主题爬虫的实现</h1>
<h2>4.1 URL队列的维护</h2><br>
<p>为了能够方便的处理链接和主题相关度的计算,需要使用4个URL队列,每个队列保存着同一处理状态的URL:<br>
a)	等待队列　在这个队列中,URL等待被爬虫处理,新发现的URL被加入到该队列。<br>
b)	错误队列　如果在下载网页时发生错误,它的URL将被加入到错误队列,一旦移入错误队列,爬虫不会对它作进一步处理。<br>
c)	抛弃队列　如果下载网页没有发生错误,且经过主题相关度的计算小于阈值,则放入该队列,一旦移入抛弃队列,爬虫不会对它作进一步处理。<br>
d)	完成队列　如果下载网页没有发生错误,如果下载网页没有发生错误,且经过主题相关度的计算大于等于阈值,就要把从中发现的URL放入等待队列,处理完毕把它加入到完成队列。</p><br>
<p>同一时间一个URL只能在一个队列中.</p>
<h2>4.2 网页爬取</h2><br>
	<p>在爬取一个网页之前，我们首先要检查该网页。如果该网页是一个网络资源，那么我们就没有必要访问，例如网页是一个mp3的下载页。我们需要忽略的网页类型有<br>
		if (s.endsWith(".zip") || s.endsWith(".gz") <br>
				|| s.endsWith(".exe") || s.endsWith(".exe")<br>
				|| s.endsWith(".jpg") || s.endsWith(".png")<br>
				|| s.endsWith(".tar") || s.endsWith(".chm")<br>
				|| s.endsWith(".iso") || s.endsWith(".gif")<br>
				|| s.endsWith(".csv") || s.endsWith(".pdf")<br>
				|| s.endsWith(".doc")|| s.endsWith(".rar"))<br>
			return false;<br>
		else<br>
			return true;<br>
	这些资源文件不值得我们下载，因为我们无法解析其内容。<br>
	在检查完网页后，我们去访问网页。如果网页的返回代码不是403或者404等拒绝或者不存在的值，我们就可以下载该网页。Java语言有相关包可以直接实现下载，不用我们自己去编写，比较方便。</p><br>

